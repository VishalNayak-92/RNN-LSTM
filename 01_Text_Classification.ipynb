{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Text Classification with RNN </center>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shreyasi25/Downloads/20191229_Batch70_CSE7321c_Lab05_RNN/txt'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "PATH = os.getcwd()\n",
    "\n",
    "TEXT_DATA_DIR = os.path.join(PATH, \"txt\")\n",
    "\n",
    "TEXT_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the list of folders inside data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kalam', 'obama', 'romney']\n"
     ]
    }
   ],
   "source": [
    "TEXT_DATA_DIR_LIST = os.listdir(TEXT_DATA_DIR)\n",
    "\n",
    "if '.DS_Store' in TEXT_DATA_DIR_LIST :\n",
    "    TEXT_DATA_DIR_LIST.remove('.DS_Store')\n",
    "\n",
    "print(TEXT_DATA_DIR_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the text data\n",
    "Iterate over the folders in which our text documents are stored, and format them into a list of documents. \n",
    "\n",
    "Also prepare a list of class indices matching the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kalam dir has following docs ['doc01.txt', 'doc02.txt', 'doc03.txt', 'doc04.txt', 'doc05.txt', 'doc06.txt', 'doc07.txt', 'doc08.txt', 'doc09.txt', 'doc10.txt', 'doc11.txt', 'doc12.txt'] \n",
      "\n",
      "obama dir has following docs ['obama01.txt', 'obama02.txt', 'obama03.txt', 'obama04.txt', 'obama05.txt', 'obama06.txt', 'obama07.txt', 'obama08.txt', 'obama09.txt', 'obama10.txt', 'obama11.txt', 'obama12.txt'] \n",
      "\n",
      "romney dir has following docs ['romney01.txt', 'romney02.txt', 'romney03.txt', 'romney04.txt', 'romney05.txt', 'romney06.txt', 'romney07.txt', 'romney08.txt', 'romney09.txt', 'romney10.txt', 'romney11.txt', 'romney12.txt'] \n",
      "\n",
      "36 docs with labels -->  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "docs = []          # list of text samples\n",
    "labels = []        # list of label ids\n",
    "labels_Index = {}  # dictionary mapping label index to label name\n",
    "\n",
    "for name in TEXT_DATA_DIR_LIST:\n",
    "    \n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    files = sorted(os.listdir(path))\n",
    "\n",
    "    print(\"{} dir has following docs {} \\n\".format( name, files ))\n",
    "\n",
    "    key = len(labels_Index)\n",
    "    labels_Index[key] = name\n",
    "\n",
    "    for fname in files:\n",
    "\n",
    "        with open( os.path.join(path, fname), encoding = \"ISO-8859-1\") as file :\n",
    "            text = file.read()\n",
    "            docs.append(text)\n",
    "\n",
    "        labels.append(key)\n",
    "\n",
    "print(len(labels), 'docs with labels --> ', labels)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Conv1D, MaxPooling1D, LSTM, Embedding, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the text samples and labels into tensors that can be fed into a neural network. \n",
    "\n",
    "To do this, we will rely on Keras utilities \n",
    "\n",
    "- keras.preprocessing.text.Tokenizer \n",
    "\n",
    "`Tokenizer` : Class for vectorizing texts, or/and turning texts into sequences (=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).\n",
    "\n",
    "`fit_on_texts(texts)` : list of texts to train on.\n",
    "        \n",
    "`word_index` : Dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7314 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(docs)\n",
    "\n",
    "word_Index = tokenizer.word_index\n",
    "\n",
    "vocab_Size = len(word_Index) + 1\n",
    "print('Found %s unique tokens.' % vocab_Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'a': 5,\n",
       " 'in': 6,\n",
       " 'that': 7,\n",
       " 'i': 8,\n",
       " 'for': 9,\n",
       " 'is': 10,\n",
       " 'we': 11,\n",
       " 'our': 12,\n",
       " 'you': 13,\n",
       " 'will': 14,\n",
       " 'it': 15,\n",
       " 'this': 16,\n",
       " 'with': 17,\n",
       " 'have': 18,\n",
       " 'on': 19,\n",
       " 'are': 20,\n",
       " 'be': 21,\n",
       " 'who': 22,\n",
       " 'as': 23,\n",
       " 'not': 24,\n",
       " 'by': 25,\n",
       " 'my': 26,\n",
       " 'from': 27,\n",
       " 'but': 28,\n",
       " 'can': 29,\n",
       " 'he': 30,\n",
       " 'has': 31,\n",
       " 'all': 32,\n",
       " 'was': 33,\n",
       " 'their': 34,\n",
       " 'they': 35,\n",
       " 'america': 36,\n",
       " 'what': 37,\n",
       " 'at': 38,\n",
       " 'or': 39,\n",
       " 'when': 40,\n",
       " 'one': 41,\n",
       " 'us': 42,\n",
       " 'an': 43,\n",
       " 'science': 44,\n",
       " 'more': 45,\n",
       " 'people': 46,\n",
       " 'so': 47,\n",
       " 'president': 48,\n",
       " 'his': 49,\n",
       " 'work': 50,\n",
       " 'nation': 51,\n",
       " 'do': 52,\n",
       " 'about': 53,\n",
       " 'me': 54,\n",
       " 'your': 55,\n",
       " 'would': 56,\n",
       " 'time': 57,\n",
       " 'world': 58,\n",
       " 'there': 59,\n",
       " 'new': 60,\n",
       " '\\x96': 61,\n",
       " 'up': 62,\n",
       " 'like': 63,\n",
       " 'which': 64,\n",
       " 'years': 65,\n",
       " 'because': 66,\n",
       " 'how': 67,\n",
       " 'been': 68,\n",
       " 'country': 69,\n",
       " 'if': 70,\n",
       " 'american': 71,\n",
       " 'am': 72,\n",
       " 'these': 73,\n",
       " 'out': 74,\n",
       " 'where': 75,\n",
       " 'many': 76,\n",
       " 'than': 77,\n",
       " 'every': 78,\n",
       " 'friends': 79,\n",
       " 'had': 80,\n",
       " 'know': 81,\n",
       " 'need': 82,\n",
       " 'just': 83,\n",
       " 'them': 84,\n",
       " 'life': 85,\n",
       " 'those': 86,\n",
       " 'students': 87,\n",
       " 'should': 88,\n",
       " 'change': 89,\n",
       " 'great': 90,\n",
       " 'jobs': 91,\n",
       " 'better': 92,\n",
       " 'children': 93,\n",
       " 'into': 94,\n",
       " 'were': 95,\n",
       " 'now': 96,\n",
       " 'must': 97,\n",
       " 'also': 98,\n",
       " 'she': 99,\n",
       " 'make': 100,\n",
       " 'americans': 101,\n",
       " 'through': 102,\n",
       " 'system': 103,\n",
       " 'young': 104,\n",
       " 'her': 105,\n",
       " 'development': 106,\n",
       " 'today': 107,\n",
       " 'first': 108,\n",
       " 'want': 109,\n",
       " 'no': 110,\n",
       " 'india': 111,\n",
       " 'let': 112,\n",
       " 'other': 113,\n",
       " 'come': 114,\n",
       " 'research': 115,\n",
       " 'future': 116,\n",
       " 'technology': 117,\n",
       " 'best': 118,\n",
       " 'get': 119,\n",
       " 'day': 120,\n",
       " 'home': 121,\n",
       " 'good': 122,\n",
       " 'take': 123,\n",
       " 'go': 124,\n",
       " 'four': 125,\n",
       " 'after': 126,\n",
       " 'only': 127,\n",
       " 'may': 128,\n",
       " 'back': 129,\n",
       " 'bihar': 130,\n",
       " 'government': 131,\n",
       " 'middle': 132,\n",
       " 'two': 133,\n",
       " 'business': 134,\n",
       " 'small': 135,\n",
       " 'its': 136,\n",
       " 'states': 137,\n",
       " 'obama': 138,\n",
       " 'last': 139,\n",
       " 'together': 140,\n",
       " 'way': 141,\n",
       " 'energy': 142,\n",
       " 'give': 143,\n",
       " 'economy': 144,\n",
       " 'here': 145,\n",
       " 'unique': 146,\n",
       " 'over': 147,\n",
       " 'campaign': 148,\n",
       " 'mission': 149,\n",
       " 'such': 150,\n",
       " 'education': 151,\n",
       " 'then': 152,\n",
       " 'never': 153,\n",
       " 'families': 154,\n",
       " 'schools': 155,\n",
       " 'scientific': 156,\n",
       " 'united': 157,\n",
       " 'even': 158,\n",
       " 'human': 159,\n",
       " 'believe': 160,\n",
       " 'thank': 161,\n",
       " 'across': 162,\n",
       " 'lead': 163,\n",
       " 'state': 164,\n",
       " 'some': 165,\n",
       " 'job': 166,\n",
       " 'john': 167,\n",
       " 'most': 168,\n",
       " 'help': 169,\n",
       " 'made': 170,\n",
       " 'say': 171,\n",
       " 'down': 172,\n",
       " 'any': 173,\n",
       " 'could': 174,\n",
       " 'national': 175,\n",
       " 'million': 176,\n",
       " 'said': 177,\n",
       " 'same': 178,\n",
       " 'free': 179,\n",
       " 'war': 180,\n",
       " 'born': 181,\n",
       " 'own': 182,\n",
       " 'hope': 183,\n",
       " 'see': 184,\n",
       " \"it's\": 185,\n",
       " 'put': 186,\n",
       " 'care': 187,\n",
       " 'promise': 188,\n",
       " 'year': 189,\n",
       " 'class': 190,\n",
       " 'areas': 191,\n",
       " 'women': 192,\n",
       " 'before': 193,\n",
       " 'god': 194,\n",
       " 'farmers': 195,\n",
       " 'election': 196,\n",
       " 'very': 197,\n",
       " 's': 198,\n",
       " 'look': 199,\n",
       " 'still': 200,\n",
       " 'right': 201,\n",
       " 'tonight': 202,\n",
       " 'knowledge': 203,\n",
       " 'build': 204,\n",
       " 'him': 205,\n",
       " 'technologies': 206,\n",
       " 'each': 207,\n",
       " 'family': 208,\n",
       " 'page': 209,\n",
       " 'become': 210,\n",
       " 'much': 211,\n",
       " 'men': 212,\n",
       " 'pay': 213,\n",
       " 'always': 214,\n",
       " 'long': 215,\n",
       " 'college': 216,\n",
       " \"don't\": 217,\n",
       " 'leadership': 218,\n",
       " 'cannot': 219,\n",
       " 'next': 220,\n",
       " 'going': 221,\n",
       " 'working': 222,\n",
       " \"that's\": 223,\n",
       " 'why': 224,\n",
       " 'earth': 225,\n",
       " 'hard': 226,\n",
       " 'power': 227,\n",
       " 'too': 228,\n",
       " 'teachers': 229,\n",
       " 'bring': 230,\n",
       " 'things': 231,\n",
       " '3': 232,\n",
       " 'less': 233,\n",
       " 'history': 234,\n",
       " 'using': 235,\n",
       " 'around': 236,\n",
       " 'success': 237,\n",
       " 'health': 238,\n",
       " 'big': 239,\n",
       " 'lives': 240,\n",
       " 'three': 241,\n",
       " 'without': 242,\n",
       " 'high': 243,\n",
       " 'course': 244,\n",
       " 'rural': 245,\n",
       " 'met': 246,\n",
       " 'nano': 247,\n",
       " 'did': 248,\n",
       " 'during': 249,\n",
       " 'another': 250,\n",
       " 'water': 251,\n",
       " 'place': 252,\n",
       " 'while': 253,\n",
       " 'dreams': 254,\n",
       " 'different': 255,\n",
       " 'man': 256,\n",
       " 'keep': 257,\n",
       " 'freedom': 258,\n",
       " \"can't\": 259,\n",
       " 'child': 260,\n",
       " 'question': 261,\n",
       " 'love': 262,\n",
       " 'economic': 263,\n",
       " '1': 264,\n",
       " 'under': 265,\n",
       " 'between': 266,\n",
       " 'challenges': 267,\n",
       " 'ever': 268,\n",
       " 'moment': 269,\n",
       " 'party': 270,\n",
       " 'school': 271,\n",
       " 'achieve': 272,\n",
       " 'agriculture': 273,\n",
       " 'again': 274,\n",
       " 'bless': 275,\n",
       " 'based': 276,\n",
       " 'stand': 277,\n",
       " 'trade': 278,\n",
       " 'paul': 279,\n",
       " 'few': 280,\n",
       " 'days': 281,\n",
       " 'being': 282,\n",
       " 'share': 283,\n",
       " '10': 284,\n",
       " 'growth': 285,\n",
       " 'well': 286,\n",
       " 'leader': 287,\n",
       " 'create': 288,\n",
       " 'five': 289,\n",
       " 'use': 290,\n",
       " 'scientists': 291,\n",
       " 'sustainable': 292,\n",
       " 'real': 293,\n",
       " 'it\\x92s': 294,\n",
       " 'washington': 295,\n",
       " 'century': 296,\n",
       " 'laboratories': 297,\n",
       " 'called': 298,\n",
       " 'information': 299,\n",
       " 'student': 300,\n",
       " '2': 301,\n",
       " '4': 302,\n",
       " '5': 303,\n",
       " 'plan': 304,\n",
       " 'seen': 305,\n",
       " 'tell': 306,\n",
       " 'fly': 307,\n",
       " \"i'm\": 308,\n",
       " 'important': 309,\n",
       " 'opportunity': 310,\n",
       " 'find': 311,\n",
       " 'centre': 312,\n",
       " 'members': 313,\n",
       " 'far': 314,\n",
       " 'end': 315,\n",
       " 'something': 316,\n",
       " 'tax': 317,\n",
       " 'vision': 318,\n",
       " 'sure': 319,\n",
       " 'indian': 320,\n",
       " 'support': 321,\n",
       " 'came': 322,\n",
       " 'poverty': 323,\n",
       " 'ready': 324,\n",
       " 'congress': 325,\n",
       " 'security': 326,\n",
       " 'companies': 327,\n",
       " 'military': 328,\n",
       " 'happy': 329,\n",
       " 'environment': 330,\n",
       " 'community': 331,\n",
       " 'particularly': 332,\n",
       " 'given': 333,\n",
       " 'off': 334,\n",
       " 'meet': 335,\n",
       " 'understand': 336,\n",
       " 'nations': 337,\n",
       " 'against': 338,\n",
       " 'fight': 339,\n",
       " 'politics': 340,\n",
       " 'east': 341,\n",
       " 'talk': 342,\n",
       " 'got': 343,\n",
       " 'generation': 344,\n",
       " 'father': 345,\n",
       " 'doing': 346,\n",
       " '000': 347,\n",
       " 'finally': 348,\n",
       " 'remembered': 349,\n",
       " 'told': 350,\n",
       " 'start': 351,\n",
       " '\\x94': 352,\n",
       " 'prosperity': 353,\n",
       " 'parents': 354,\n",
       " 'kind': 355,\n",
       " 'innovation': 356,\n",
       " 'needs': 357,\n",
       " 'experience': 358,\n",
       " 'face': 359,\n",
       " 'convergence': 360,\n",
       " 'cost': 361,\n",
       " 'think': 362,\n",
       " 'choice': 363,\n",
       " 'virtual': 364,\n",
       " 'america\\x92s': 365,\n",
       " 'youth': 366,\n",
       " 'peace': 367,\n",
       " 'responsibility': 368,\n",
       " 'once': 369,\n",
       " 'remember': 370,\n",
       " 'provide': 371,\n",
       " 'sciences': 372,\n",
       " 'since': 373,\n",
       " 'done': 374,\n",
       " 'thought': 375,\n",
       " 'little': 376,\n",
       " 'clear': 377,\n",
       " 'mccain': 378,\n",
       " 'debt': 379,\n",
       " 'iraq': 380,\n",
       " 'bill': 381,\n",
       " 'leaders': 382,\n",
       " 'systems': 383,\n",
       " 'ask': 384,\n",
       " 'others': 385,\n",
       " 'spirit': 386,\n",
       " 'times': 387,\n",
       " 'bio': 388,\n",
       " 'both': 389,\n",
       " 'policies': 390,\n",
       " 'mobile': 391,\n",
       " 'office': 392,\n",
       " 'away': 393,\n",
       " 'workers': 394,\n",
       " 'afford': 395,\n",
       " 'credit': 396,\n",
       " 'building': 397,\n",
       " 'citizens': 398,\n",
       " 'action': 399,\n",
       " 'prof': 400,\n",
       " 'communication': 401,\n",
       " '7': 402,\n",
       " '9': 403,\n",
       " 'worked': 404,\n",
       " 'land': 405,\n",
       " 'able': 406,\n",
       " 'saw': 407,\n",
       " 'whether': 408,\n",
       " 'confidence': 409,\n",
       " 'ago': 410,\n",
       " 'common': 411,\n",
       " 'went': 412,\n",
       " 'living': 413,\n",
       " 'minds': 414,\n",
       " 'model': 415,\n",
       " 'does': 416,\n",
       " 'among': 417,\n",
       " 'nuclear': 418,\n",
       " 'higher': 419,\n",
       " 'values': 420,\n",
       " 'making': 421,\n",
       " 'second': 422,\n",
       " 'courage': 423,\n",
       " 'night': 424,\n",
       " 'comes': 425,\n",
       " 'towards': 426,\n",
       " 'learning': 427,\n",
       " 'dear': 428,\n",
       " 'past': 429,\n",
       " 'ourselves': 430,\n",
       " 'laboratory': 431,\n",
       " 'potential': 432,\n",
       " 'might': 433,\n",
       " 'side': 434,\n",
       " 'old': 435,\n",
       " 'resources': 436,\n",
       " 'bankruptcy': 437,\n",
       " 'quality': 438,\n",
       " 'leading': 439,\n",
       " 'millions': 440,\n",
       " 'part': 441,\n",
       " 'willing': 442,\n",
       " 'foundation': 443,\n",
       " 'created': 444,\n",
       " 'asked': 445,\n",
       " 'took': 446,\n",
       " 'nearly': 447,\n",
       " 'words': 448,\n",
       " 'problem': 449,\n",
       " 'started': 450,\n",
       " 'house': 451,\n",
       " 'means': 452,\n",
       " 'turn': 453,\n",
       " 'enterprise': 454,\n",
       " 'faith': 455,\n",
       " 'kids': 456,\n",
       " 'taxes': 457,\n",
       " 'dr': 458,\n",
       " 'role': 459,\n",
       " 'used': 460,\n",
       " 'heart': 461,\n",
       " 'political': 462,\n",
       " 'thing': 463,\n",
       " 'mother': 464,\n",
       " 'benefits': 465,\n",
       " 'path': 466,\n",
       " 'succeed': 467,\n",
       " 'answer': 468,\n",
       " 'clean': 469,\n",
       " 'live': 470,\n",
       " 'yes': 471,\n",
       " 'iit': 472,\n",
       " 'lab': 473,\n",
       " 'full': 474,\n",
       " 'greatest': 475,\n",
       " 'senator': 476,\n",
       " 'chance': 477,\n",
       " 'businesses': 478,\n",
       " 'deserve': 479,\n",
       " 'cut': 480,\n",
       " 'promised': 481,\n",
       " 'moral': 482,\n",
       " 'teacher': 483,\n",
       " 'developed': 484,\n",
       " 'says': 485,\n",
       " 'thinking': 486,\n",
       " 'ahead': 487,\n",
       " 'value': 488,\n",
       " 'society': 489,\n",
       " '8': 490,\n",
       " 'material': 491,\n",
       " 'bose': 492,\n",
       " 'basic': 493,\n",
       " 'greater': 494,\n",
       " 'thousands': 495,\n",
       " 'tomorrow': 496,\n",
       " 'billion': 497,\n",
       " 'months': 498,\n",
       " 'experiments': 499,\n",
       " 'money': 500,\n",
       " 'offer': 501,\n",
       " 'lost': 502,\n",
       " 'honor': 503,\n",
       " 'republicans': 504,\n",
       " 'oil': 505,\n",
       " 'number': 506,\n",
       " 'voice': 507,\n",
       " 'management': 508,\n",
       " 'excellence': 509,\n",
       " 'gave': 510,\n",
       " 'space': 511,\n",
       " 'food': 512,\n",
       " 'risk': 513,\n",
       " 'room': 514,\n",
       " 'visited': 515,\n",
       " 'field': 516,\n",
       " 'company': 517,\n",
       " 'income': 518,\n",
       " 'problems': 519,\n",
       " 'particle': 520,\n",
       " 'labs': 521,\n",
       " 'ideas': 522,\n",
       " 'else': 523,\n",
       " 'white': 524,\n",
       " 'patna': 525,\n",
       " 'service': 526,\n",
       " 'join': 527,\n",
       " 'protect': 528,\n",
       " 'governor': 529,\n",
       " 'democrats': 530,\n",
       " 'federal': 531,\n",
       " 'gujarat': 532,\n",
       " 'dream': 533,\n",
       " 'integrated': 534,\n",
       " 'mind': 535,\n",
       " 'sun': 536,\n",
       " 'led': 537,\n",
       " 'light': 538,\n",
       " 'respect': 539,\n",
       " 'self': 540,\n",
       " 'design': 541,\n",
       " 'team': 542,\n",
       " '6': 543,\n",
       " 'developing': 544,\n",
       " 'missions': 545,\n",
       " 'productivity': 546,\n",
       " 'already': 547,\n",
       " 'instead': 548,\n",
       " 'mr': 549,\n",
       " 'planet': 550,\n",
       " 'rise': 551,\n",
       " 'behind': 552,\n",
       " 'dollars': 553,\n",
       " 'woman': 554,\n",
       " 'restore': 555,\n",
       " \"i've\": 556,\n",
       " 'ryan': 557,\n",
       " 'obamacare': 558,\n",
       " 'makes': 559,\n",
       " 'products': 560,\n",
       " 'countries': 561,\n",
       " 'pura': 562,\n",
       " 'reach': 563,\n",
       " 'call': 564,\n",
       " 'story': 565,\n",
       " 'creating': 566,\n",
       " 'data': 567,\n",
       " 'south': 568,\n",
       " 'fellow': 569,\n",
       " 'leave': 570,\n",
       " 'left': 571,\n",
       " 'bush': 572,\n",
       " 'often': 573,\n",
       " 'clinton': 574,\n",
       " 'don\\x92t': 575,\n",
       " 'public': 576,\n",
       " '15': 577,\n",
       " 'safe': 578,\n",
       " 'realize': 579,\n",
       " 'progress': 580,\n",
       " 'due': 581,\n",
       " 'programme': 582,\n",
       " 'physics': 583,\n",
       " 'stories': 584,\n",
       " 'above': 585,\n",
       " 'really': 586,\n",
       " 'sense': 587,\n",
       " 'cause': 588,\n",
       " 'secure': 589,\n",
       " 'record': 590,\n",
       " 'difficult': 591,\n",
       " 'villages': 592,\n",
       " 'study': 593,\n",
       " 'having': 594,\n",
       " 'multiple': 595,\n",
       " 'possible': 596,\n",
       " 'single': 597,\n",
       " 'purpose': 598,\n",
       " 'shape': 599,\n",
       " 'scientist': 600,\n",
       " 'known': 601,\n",
       " 'global': 602,\n",
       " 'strength': 603,\n",
       " 'taking': 604,\n",
       " 'journey': 605,\n",
       " 'village': 606,\n",
       " 'law': 607,\n",
       " 'defeat': 608,\n",
       " 'win': 609,\n",
       " 'city': 610,\n",
       " 'choose': 611,\n",
       " 'homes': 612,\n",
       " 'afghanistan': 613,\n",
       " 'virginia': 614,\n",
       " 'attacks': 615,\n",
       " 'spending': 616,\n",
       " 'begin': 617,\n",
       " 'prosperous': 618,\n",
       " 'example': 619,\n",
       " 'universe': 620,\n",
       " 'individual': 621,\n",
       " 'raman': 622,\n",
       " 'found': 623,\n",
       " 'healthcare': 624,\n",
       " 'including': 625,\n",
       " 'network': 626,\n",
       " 'set': 627,\n",
       " 'asking': 628,\n",
       " 'message': 629,\n",
       " 'questions': 630,\n",
       " 'results': 631,\n",
       " 'efforts': 632,\n",
       " 'area': 633,\n",
       " 'agricultural': 634,\n",
       " 'brought': 635,\n",
       " 'focus': 636,\n",
       " 'social': 637,\n",
       " 'proud': 638,\n",
       " 'name': 639,\n",
       " 'international': 640,\n",
       " 'product': 641,\n",
       " 'solar': 642,\n",
       " 'beautiful': 643,\n",
       " 'yourself': 644,\n",
       " 'natural': 645,\n",
       " 'stop': 646,\n",
       " 'son': 647,\n",
       " 'half': 648,\n",
       " 'yet': 649,\n",
       " 'fought': 650,\n",
       " 'candidate': 651,\n",
       " 'liberty': 652,\n",
       " 'open': 653,\n",
       " 'calls': 654,\n",
       " 'defense': 655,\n",
       " \"we've\": 656,\n",
       " 'failed': 657,\n",
       " 'percent': 658,\n",
       " 'budget': 659,\n",
       " 'immigration': 660,\n",
       " 'aid': 661,\n",
       " '2012': 662,\n",
       " 'strong': 663,\n",
       " 'hours': 664,\n",
       " 'ensure': 665,\n",
       " 'matter': 666,\n",
       " 'e': 667,\n",
       " 'independence': 668,\n",
       " 'present': 669,\n",
       " 'creative': 670,\n",
       " 'learn': 671,\n",
       " 'third': 672,\n",
       " 'benefit': 673,\n",
       " 'institutions': 674,\n",
       " 'grow': 675,\n",
       " 'act': 676,\n",
       " 'point': 677,\n",
       " 'thoughts': 678,\n",
       " 'sector': 679,\n",
       " 'project': 680,\n",
       " 'further': 681,\n",
       " 'wanted': 682,\n",
       " 'short': 683,\n",
       " '20': 684,\n",
       " 'gas': 685,\n",
       " 'happen': 686,\n",
       " 'george': 687,\n",
       " 'large': 688,\n",
       " 'wings': 689,\n",
       " 'nothing': 690,\n",
       " 'break': 691,\n",
       " 'lived': 692,\n",
       " 'anything': 693,\n",
       " 'spent': 694,\n",
       " 'began': 695,\n",
       " 'agree': 696,\n",
       " 'can\\x92t': 697,\n",
       " 'vote': 698,\n",
       " 'shared': 699,\n",
       " 'lot': 700,\n",
       " 'stood': 701,\n",
       " 'someone': 702,\n",
       " 'promises': 703,\n",
       " '\\x97': 704,\n",
       " 'fired': 705,\n",
       " \"doesn't\": 706,\n",
       " 'send': 707,\n",
       " 'principals': 708,\n",
       " 'decade': 709,\n",
       " 'teaching': 710,\n",
       " 'alone': 711,\n",
       " 'various': 712,\n",
       " 'person': 713,\n",
       " 'blue': 714,\n",
       " 'giving': 715,\n",
       " 'understanding': 716,\n",
       " 'theory': 717,\n",
       " 'true': 718,\n",
       " 'capacity': 719,\n",
       " '2020': 720,\n",
       " 'ability': 721,\n",
       " 'process': 722,\n",
       " 'play': 723,\n",
       " 'providing': 724,\n",
       " 'needed': 725,\n",
       " 'running': 726,\n",
       " 'hold': 727,\n",
       " 'level': 728,\n",
       " 'meeting': 729,\n",
       " 'within': 730,\n",
       " 'green': 731,\n",
       " 'transparent': 732,\n",
       " 'sn': 733,\n",
       " 'discovery': 734,\n",
       " 'longer': 735,\n",
       " 'hand': 736,\n",
       " 'age': 737,\n",
       " 'continue': 738,\n",
       " 'takes': 739,\n",
       " 'trust': 740,\n",
       " 'ones': 741,\n",
       " 'hear': 742,\n",
       " 'wants': 743,\n",
       " 'duty': 744,\n",
       " 'street': 745,\n",
       " 'ieee': 746,\n",
       " 'generations': 747,\n",
       " 'he\\x92s': 748,\n",
       " 'harder': 749,\n",
       " 'measure': 750,\n",
       " 'veterans': 751,\n",
       " 're': 752,\n",
       " 'iran': 753,\n",
       " 'foreign': 754,\n",
       " 'elected': 755,\n",
       " 'decades': 756,\n",
       " 'taken': 757,\n",
       " 'application': 758,\n",
       " 'transform': 759,\n",
       " '12': 760,\n",
       " 'suggest': 761,\n",
       " 'sea': 762,\n",
       " 'b': 763,\n",
       " 'heard': 764,\n",
       " 'hour': 765,\n",
       " 'mathematician': 766,\n",
       " 'essential': 767,\n",
       " 'road': 768,\n",
       " 'themselves': 769,\n",
       " 'tools': 770,\n",
       " 'perfect': 771,\n",
       " 'commitment': 772,\n",
       " 'myself': 773,\n",
       " 'conditions': 774,\n",
       " 'entire': 775,\n",
       " 'district': 776,\n",
       " 'rate': 777,\n",
       " 'region': 778,\n",
       " 'efficiency': 779,\n",
       " 'entrepreneurs': 780,\n",
       " 'policy': 781,\n",
       " 'seek': 782,\n",
       " 'term': 783,\n",
       " 'establish': 784,\n",
       " 'goal': 785,\n",
       " 'bringing': 786,\n",
       " 'affected': 787,\n",
       " 'applied': 788,\n",
       " 'assembly': 789,\n",
       " 'deficit': 790,\n",
       " 'societal': 791,\n",
       " 'anyone': 792,\n",
       " 'fundamental': 793,\n",
       " 'fighting': 794,\n",
       " 'forces': 795,\n",
       " 'run': 796,\n",
       " 'works': 797,\n",
       " 'north': 798,\n",
       " 'raise': 799,\n",
       " 'republican': 800,\n",
       " 'forward': 801,\n",
       " 'ann': 802,\n",
       " 'struggle': 803,\n",
       " 'speak': 804,\n",
       " 'bad': 805,\n",
       " 'invest': 806,\n",
       " \"we're\": 807,\n",
       " 'mom': 808,\n",
       " 'kerry': 809,\n",
       " 'unemployment': 810,\n",
       " 'medicare': 811,\n",
       " '21st': 812,\n",
       " 'everyone': 813,\n",
       " 'gone': 814,\n",
       " 'creativity': 815,\n",
       " 'whole': 816,\n",
       " 'black': 817,\n",
       " 'looking': 818,\n",
       " 'morning': 819,\n",
       " 'culture': 820,\n",
       " 'indeed': 821,\n",
       " 'certain': 822,\n",
       " 'critical': 823,\n",
       " 'teach': 824,\n",
       " 'parts': 825,\n",
       " 'feel': 826,\n",
       " 'larger': 827,\n",
       " 'shaping': 828,\n",
       " 'production': 829,\n",
       " 'farming': 830,\n",
       " 'produce': 831,\n",
       " 'increase': 832,\n",
       " 'services': 833,\n",
       " 'insurance': 834,\n",
       " 'track': 835,\n",
       " 'revolution': 836,\n",
       " 'successful': 837,\n",
       " 'ground': 838,\n",
       " 'failure': 839,\n",
       " 'competitiveness': 840,\n",
       " 'index': 841,\n",
       " 'engineering': 842,\n",
       " 'effective': 843,\n",
       " 'boy': 844,\n",
       " 'nobel': 845,\n",
       " 'felt': 846,\n",
       " 'turned': 847,\n",
       " 'flag': 848,\n",
       " 'agastya': 849,\n",
       " 'hands': 850,\n",
       " 'aircraft': 851,\n",
       " 'built': 852,\n",
       " 'strategy': 853,\n",
       " 'knew': 854,\n",
       " 'pursue': 855,\n",
       " 'employment': 856,\n",
       " 'advice': 857,\n",
       " 'try': 858,\n",
       " 'dignity': 859,\n",
       " 'poor': 860,\n",
       " 'that\\x92s': 861,\n",
       " 'solve': 862,\n",
       " 'ideals': 863,\n",
       " 'reform': 864,\n",
       " 'nor': 865,\n",
       " 'rights': 866,\n",
       " 'principles': 867,\n",
       " 'we\\x92ll': 868,\n",
       " 'wrong': 869,\n",
       " 'served': 870,\n",
       " 'talking': 871,\n",
       " 'sacrifice': 872,\n",
       " \"you're\": 873,\n",
       " \"didn't\": 874,\n",
       " \"they're\": 875,\n",
       " 'allies': 876,\n",
       " \"there's\": 877,\n",
       " 'applause': 878,\n",
       " 'i\\x92ve': 879,\n",
       " 'guard': 880,\n",
       " 'becoming': 881,\n",
       " 'primary': 882,\n",
       " 'particular': 883,\n",
       " 'discovered': 884,\n",
       " 'sir': 885,\n",
       " 'einstein': 886,\n",
       " 'st': 887,\n",
       " 'became': 888,\n",
       " 'fair': 889,\n",
       " 'special': 890,\n",
       " 'districts': 891,\n",
       " 'electric': 892,\n",
       " 'per': 893,\n",
       " 'cities': 894,\n",
       " 'difference': 895,\n",
       " 'pyramid': 896,\n",
       " 'rich': 897,\n",
       " 'technological': 898,\n",
       " 'd': 899,\n",
       " 'simple': 900,\n",
       " 'market': 901,\n",
       " 'association': 902,\n",
       " 'later': 903,\n",
       " 'early': 904,\n",
       " 'vital': 905,\n",
       " 'line': 906,\n",
       " 'sent': 907,\n",
       " 'available': 908,\n",
       " 'travel': 909,\n",
       " 'greetings': 910,\n",
       " 'china': 911,\n",
       " 'industry': 912,\n",
       " 'deliver': 913,\n",
       " 'treatment': 914,\n",
       " 'september': 915,\n",
       " 'low': 916,\n",
       " 'hopes': 917,\n",
       " 'recent': 918,\n",
       " 'save': 919,\n",
       " 'standing': 920,\n",
       " 'friend': 921,\n",
       " 'whom': 922,\n",
       " 'lady': 923,\n",
       " 'everybody': 924,\n",
       " 'plans': 925,\n",
       " 'serve': 926,\n",
       " 'brave': 927,\n",
       " 'upon': 928,\n",
       " 'lines': 929,\n",
       " 'forget': 930,\n",
       " 'threats': 931,\n",
       " 'democratic': 932,\n",
       " 'union': 933,\n",
       " 'rather': 934,\n",
       " 'private': 935,\n",
       " 'card': 936,\n",
       " 'corporations': 937,\n",
       " 'troops': 938,\n",
       " 'israel': 939,\n",
       " 'amendment': 940,\n",
       " 'believes': 941,\n",
       " 'helped': 942,\n",
       " 'romney': 943,\n",
       " 'dad': 944,\n",
       " 'participate': 945,\n",
       " 'vice': 946,\n",
       " 'birth': 947,\n",
       " 'midst': 948,\n",
       " 'topic': 949,\n",
       " 'capital': 950,\n",
       " 'non': 951,\n",
       " 'organization': 952,\n",
       " 'case': 953,\n",
       " 'famous': 954,\n",
       " 'weapons': 955,\n",
       " 'type': 956,\n",
       " 'walk': 957,\n",
       " 'mathematics': 958,\n",
       " 'spoke': 959,\n",
       " 'skills': 960,\n",
       " 'beyond': 961,\n",
       " 'move': 962,\n",
       " 'latest': 963,\n",
       " 'throughout': 964,\n",
       " 'challenge': 965,\n",
       " 'conclusion': 966,\n",
       " 'oath': 967,\n",
       " 'itself': 968,\n",
       " '11': 969,\n",
       " 'spread': 970,\n",
       " 'shri': 971,\n",
       " 'farm': 972,\n",
       " 'task': 973,\n",
       " 'following': 974,\n",
       " 'aim': 975,\n",
       " '500': 976,\n",
       " 'achieved': 977,\n",
       " 'major': 978,\n",
       " 'integrity': 979,\n",
       " 'reciprocating': 980,\n",
       " 'equal': 981,\n",
       " 'size': 982,\n",
       " 'electronics': 983,\n",
       " 'disease': 984,\n",
       " 'invention': 985,\n",
       " 'march': 986,\n",
       " 'removing': 987,\n",
       " 'moon': 988,\n",
       " 'learned': 989,\n",
       " 'greatness': 990,\n",
       " 'hurricane': 991,\n",
       " 'katrina': 992,\n",
       " 'broken': 993,\n",
       " 'partner': 994,\n",
       " 'chief': 995,\n",
       " 'retirement': 996,\n",
       " 'programs': 997,\n",
       " 'fear': 998,\n",
       " 'wish': 999,\n",
       " 'profile': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`texts_to_sequences(texts)` : Takes list of texts to turn to sequences, returns list of sequences (one per text input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "September 21, 2012, Written words nor my own spoken words will never and can never honestly express nor explain exactly how I feel about Barack H. Obama. Obama is so EVIL, wrotten and wicked. The most horrid Liar that I have ever seen in my whole life. And how he ever got into congress and to the Presidency in the first place is so unbelievable to me. It is a horrible shock to my whle body actually. Obama is not in any way a President. George Washington and John Adams and Abraham Lincoln would not believe it either if they could see what has become of America, now. I saw a lady who told me that she is going to vote for Obama. I told her that Obama is a Marxist-Communist and that he Murders Babies in the Womb and that he wants to ruin America and that he hates America, GOD and Israel. I told her that Obama is a Dictator and that he wants to bring in Sharia Law and that he will bring us all into Misery. But she did not believe me, she thought that I was crazy to be wearing a ROMNEY T-shirt. Some people will just not ever understand, nor change their minds, from the idiotic television, media brainwashing back into the GOD-given Common-Sense that they were suppose to be born with. What do we do with people who refuse to listen??? I do not want to be under The New World Order or sent into deep Darkness just because so many people want to ruin Life for the rest of US. GOD Bless Mitt Romney and Paul Ryan",
      "",
      ". \n",
      " [915, 3695, 662, 1610, 448, 865, 26, 182, 1552, 448, 14, 153, 2, 29, 153, 7227, 1533, 865, 1470, 1227, 67, 8, 826, 53, 1278, 1475, 138, 138, 10, 47, 1408, 7228, 2, 7229, 1, 168, 7230, 7231, 7, 8, 18, 268, 305, 6, 26, 816, 85, 2, 67, 30, 268, 343, 94, 325, 2, 3, 1, 1265, 6, 1, 108, 252, 10, 47, 7232, 3, 54, 15, 10, 5, 7233, 2485, 3, 26, 7234, 2507, 1070, 138, 10, 24, 6, 173, 141, 5, 48, 687, 295, 2, 167, 7235, 2, 2322, 1912, 56, 24, 160, 15, 1464, 70, 35, 174, 184, 37, 31, 210, 4, 36, 96, 8, 407, 5, 923, 22, 350, 54, 7, 99, 10, 221, 3, 698, 9, 138, 8, 350, 105, 7, 138, 10, 5, 7236, 7237, 2, 7, 30, 7238, 7239, 6, 1, 7240, 2, 7, 30, 743, 3, 4161, 36, 2, 7, 30, 7241, 36, 194, 2, 939, 8, 350, 105, 7, 138, 10, 5, 3783, 2, 7, 30, 743, 3, 230, 6, 3993, 607, 2, 7, 30, 14, 230, 42, 32, 94, 3299, 28, 99, 248, 24, 160, 54, 99, 375, 7, 8, 33, 3925, 3, 21, 2418, 5, 943, 1166, 7242, 165, 46, 14, 83, 24, 268, 336, 865, 89, 34, 414, 27, 1, 7243, 1596, 1091, 7244, 129, 94, 1, 194, 333, 411, 587, 7, 35, 95, 7245, 3, 21, 181, 17, 37, 52, 11, 52, 17, 46, 22, 3917, 3, 1372, 8, 52, 24, 109, 3, 21, 265, 1, 60, 58, 1437, 39, 907, 94, 1044, 1281, 83, 66, 47, 76, 46, 109, 3, 4161, 85, 9, 1, 1107, 4, 42, 194, 275, 7246, 943, 2, 279, 7247]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "sequences = tokenizer.texts_to_sequences(docs)\n",
    "print(docs[33], '\\n', sequences[33])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the length of each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(279, 33),\n",
       " (289, 9),\n",
       " (453, 7),\n",
       " (677, 24),\n",
       " (809, 34),\n",
       " (812, 18),\n",
       " (830, 23),\n",
       " (1025, 11),\n",
       " (1141, 35),\n",
       " (1201, 4),\n",
       " (1582, 21),\n",
       " (1742, 22),\n",
       " (1877, 14),\n",
       " (1879, 32),\n",
       " (1884, 20),\n",
       " (1981, 12),\n",
       " (2168, 19),\n",
       " (2203, 28),\n",
       " (2315, 30),\n",
       " (2382, 17),\n",
       " (2398, 13),\n",
       " (2484, 16),\n",
       " (2486, 25),\n",
       " (2510, 10),\n",
       " (2690, 26),\n",
       " (2734, 29),\n",
       " (2950, 5),\n",
       " (2956, 2),\n",
       " (3000, 0),\n",
       " (3049, 8),\n",
       " (3153, 6),\n",
       " (3185, 1),\n",
       " (3411, 27),\n",
       " (4110, 31),\n",
       " (4123, 3),\n",
       " (4676, 15)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(len(d), i) for i, d in enumerate(sequences)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras.preprocessing.sequence.pad_sequences\n",
    "\n",
    "The sequences have different lengths and Keras requires inputs to be vectorized and all inputs to have the same length. \n",
    "\n",
    "We will pad all input sequences to have the length of 1000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (36, 1000)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "data = pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default `truncating='pre'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[292, 106, 5267, 3, 5, 1471, 4, 159, 106, 6, 64, 1176, 290, 3529, 3, 335, 159, 357, 253, 5268, 1, 330, 47, 7, 73, 357, 29, 21, 246, 24, 127, 6, 1, 669, 28, 98, 9, 1, 747, 3, 114, 79, 58, 31, 47, 314, 305, 1, 106, 5269, 106, 2, 637, 106, 263, 106, 2, 462, 106, 28, 581, 3, 1, 3530, 4, 645, 436, 2, 5270, 1106, 439, 3, 45, 77, 402, 497, 6, 1, 58, 107, 15, 10, 767, 3, 362, 4, 292, 106, 6, 78, 1161, 4, 159, 85, 6, 78, 679, 4, 1, 144, 47, 314, 211, 4, 1, 567, 282, 1299, 25, 1, 337, 235, 1, 206, 484, 2, 203, 3010, 102, 712, 452, 4, 567, 2134, 147, 2135, 23, 286, 23, 511, 401, 3132, 8, 33, 628, 773, 37, 11, 671, 27, 47, 76, 2135, 2, 391, 3531, 2311, 9, 1164, 1905, 2, 401, 47, 76, 838, 5271, 47, 76, 2505, 383, 2, 47, 211, 4, 567, 2076, 27, 712, 567, 5272, 103, 40, 11, 2667, 1, 567, 37, 10, 1, 5273, 7, 29, 21, 333, 3, 1, 245, 1106, 4, 232, 497, 46, 3, 1072, 84, 17, 299, 203, 2, 2081, 3, 1322, 34, 182, 438, 4, 85, 8, 33, 486, 37, 14, 21, 1, 146, 1472, 7, 14, 169, 3, 272, 292, 106, 8, 56, 63, 3, 2136, 43, 534, 1094, 9, 292, 106, 102, 5, 146, 415, 1059, 25, 54, 298, 2137, 331, 896, 64, 2668, 1, 360, 4, 206, 9, 786, 106, 3, 1, 46, 235, 645, 436, 3532, 9, 1, 747, 3, 114, 79, 112, 54, 96, 143, 26, 1114, 4, 1, 292, 106, 415, 235, 1, 360, 4, 206, 9, 1164, 191, 106, 625, 245, 191, 2540, 232, 497, 1106, 5, 1099, 2000, 232, 497, 31, 68, 2323, 276, 19, 1, 585, 2940, 8, 56, 63, 3, 669, 26, 1114, 4, 2137, 331, 896, 2471, 3464, 1, 974, 264, 645, 436, 301, 299, 2, 401, 232, 360, 4, 206, 302, 791, 134, 415, 303, 1202, 543, 2138, 96, 112, 54, 1086, 6, 2607, 19, 67, 3, 272, 292, 106, 6, 5, 333, 2137, 331, 896, 3533, 37, 10, 1, 2427, 4, 3533, 59, 20, 76, 175, 1635, 38, 1, 838, 728, 17, 1, 169, 4, 206, 3, 528, 1, 330, 2, 230, 292, 106, 1983, 9, 578, 3534, 251, 2, 251, 9, 1456, 2139, 1, 1067, 235, 117, 2, 118, 1033, 5274, 45, 1342, 142, 436, 3, 1309, 1, 5275, 19, 1, 2537, 1066, 5276, 1, 391, 436, 47, 7, 15, 416, 24, 2669, 1, 330, 2, 163, 3, 681, 3086, 4, 238, 2, 330, 2, 1714, 1, 388, 2109, 2023, 230, 367, 2, 263, 353, 3, 1, 51, 67, 3, 52, 7, 199, 38, 1, 1068, 4, 1, 3535, 1068, 4, 1, 896, 1, 2138, 20, 298, 1, 1068, 4, 1, 896, 6, 1, 2137, 331, 896, 2, 35, 20, 1, 905, 5277, 9, 32, 263, 3139, 2, 1, 5278, 4, 292, 106, 436, 645, 436, 20, 1, 3103, 9, 292, 106, 11, 18, 645, 436, 150, 23, 762, 405, 251, 1313, 3536, 5279, 2, 1182, 89, 1, 292, 106, 6, 78, 1161, 4, 1, 585, 10, 767, 6, 2113, 2, 2670, 1, 330, 11, 18, 68, 235, 1, 645, 436, 9, 1, 159, 106, 235, 1, 44, 2, 117, 2, 136, 1202, 28, 38, 1, 178, 57, 2671, 1, 330, 6, 1, 1474, 4, 5280, 5281, 3492, 2671, 1, 405, 762, 2, 1298, 6, 1, 1474, 4, 3461, 5282, 1065, 3042, 107, 645, 436, 20, 3530, 2, 1, 330, 10, 3114, 439, 3, 602, 3537, 360, 4, 206, 107, 581, 3, 1, 360, 4, 206, 150, 23, 388, 1194, 247, 2, 1450, 206, 3168, 94, 469, 2, 731, 206, 1424, 3, 595, 560, 2, 383, 6, 251, 142, 330, 1067, 1065, 2, 2672, 2, 624, 9, 619, 642, 206, 18, 333, 1, 108, 1718, 5283, 642, 1494, 6, 532, 247, 5284, 117, 31, 333, 1, 578, 3534, 251, 2140, 247, 3538, 2, 1450, 206, 31, 333, 1, 388, 5285, 3538, 2140, 115, 2, 106, 10, 1473, 2645, 235, 1, 360, 4, 206, 64, 14, 143, 1, 469, 2, 731, 560, 3, 1, 1333, 96, 67, 3, 665, 7, 73, 206, 2092, 1, 74, 1353, 235, 299, 2, 401, 206, 2, 1716, 791, 134, 415, 299, 2, 401, 299, 2134, 344, 2, 5286, 102, 401, 626, 2, 136, 206, 147, 2135, 2, 1043, 2628, 31, 3154, 60, 1709, 581, 3, 1, 360, 4, 2673, 2674, 206, 16, 1183, 3, 5287, 2, 835, 1, 645, 436, 1183, 3, 304, 9, 1636, 1, 330, 2, 5288, 1, 2672, 1, 299, 2675, 2, 136, 2132, 14, 759, 94, 203, 299, 2, 3523, 383, 14, 3485, 567, 27, 1, 405, 2, 511, 102, 1, 2135, 626, 2, 2538, 206, 1, 5289, 5290, 2, 2673, 2674, 206, 235, 1043, 626, 128, 1924, 2, 2667, 1, 567, 27, 1164, 1905, 1176, 3112, 19, 405, 251, 2, 762, 1286, 2, 466, 4, 1, 1298, 1307, 102, 1, 2311, 150, 23, 5291, 2, 5292, 1652, 2673, 2674, 5293, 770, 128, 2667, 1, 567, 27, 567, 2675, 4, 1, 1118, 4, 567, 1299, 64, 128, 1714, 1, 203, 19, 67, 3, 230, 292, 106, 6, 595, 191, 150, 23, 1065, 1067, 142, 5294, 2, 2672, 235, 1, 299, 2, 401, 206, 11, 82, 3, 1331, 1179, 791, 134, 415, 47, 7, 1, 115, 631, 4, 360, 4, 206, 20, 460, 9, 159, 106, 6, 5, 292, 141, 791, 134, 415, 38, 41, 434, 117, 276, 383, 106, 1461, 74, 4, 156, 115, 2, 38, 113, 434, 1716, 43, 1179, 134, 415, 64, 14, 123, 1, 206, 3, 1, 2138, 9, 566, 292, 106, 383, 3497, 263, 1202, 27, 1, 146, 791, 134, 415, 64, 14, 1072, 2, 1714, 1, 2138, 150, 23, 195, 2408, 2625, 394, 46, 413, 6, 1, 245, 191, 40, 1, 292, 106, 791, 134, 415, 10, 788, 7, 14, 1186, 94, 1, 290, 4, 908, 645, 436, 3532, 5295, 15, 242, 2671, 1, 330, 2, 38, 1, 178, 57, 100, 15, 908, 9, 1, 747, 3, 114, 47, 7, 602, 367, 2, 353, 10, 1935] \n",
      "\n",
      " [ 357   29   21  246   24  127    6    1  669   28   98    9    1  747\n",
      "    3  114   79   58   31   47  314  305    1  106 5269  106    2  637\n",
      "  106  263  106    2  462  106   28  581    3    1 3530    4  645  436\n",
      "    2 5270 1106  439    3   45   77  402  497    6    1   58  107   15\n",
      "   10  767    3  362    4  292  106    6   78 1161    4  159   85    6\n",
      "   78  679    4    1  144   47  314  211    4    1  567  282 1299   25\n",
      "    1  337  235    1  206  484    2  203 3010  102  712  452    4  567\n",
      " 2134  147 2135   23  286   23  511  401 3132    8   33  628  773   37\n",
      "   11  671   27   47   76 2135    2  391 3531 2311    9 1164 1905    2\n",
      "  401   47   76  838 5271   47   76 2505  383    2   47  211    4  567\n",
      " 2076   27  712  567 5272  103   40   11 2667    1  567   37   10    1\n",
      " 5273    7   29   21  333    3    1  245 1106    4  232  497   46    3\n",
      " 1072   84   17  299  203    2 2081    3 1322   34  182  438    4   85\n",
      "    8   33  486   37   14   21    1  146 1472    7   14  169    3  272\n",
      "  292  106    8   56   63    3 2136   43  534 1094    9  292  106  102\n",
      "    5  146  415 1059   25   54  298 2137  331  896   64 2668    1  360\n",
      "    4  206    9  786  106    3    1   46  235  645  436 3532    9    1\n",
      "  747    3  114   79  112   54   96  143   26 1114    4    1  292  106\n",
      "  415  235    1  360    4  206    9 1164  191  106  625  245  191 2540\n",
      "  232  497 1106    5 1099 2000  232  497   31   68 2323  276   19    1\n",
      "  585 2940    8   56   63    3  669   26 1114    4 2137  331  896 2471\n",
      " 3464    1  974  264  645  436  301  299    2  401  232  360    4  206\n",
      "  302  791  134  415  303 1202  543 2138   96  112   54 1086    6 2607\n",
      "   19   67    3  272  292  106    6    5  333 2137  331  896 3533   37\n",
      "   10    1 2427    4 3533   59   20   76  175 1635   38    1  838  728\n",
      "   17    1  169    4  206    3  528    1  330    2  230  292  106 1983\n",
      "    9  578 3534  251    2  251    9 1456 2139    1 1067  235  117    2\n",
      "  118 1033 5274   45 1342  142  436    3 1309    1 5275   19    1 2537\n",
      " 1066 5276    1  391  436   47    7   15  416   24 2669    1  330    2\n",
      "  163    3  681 3086    4  238    2  330    2 1714    1  388 2109 2023\n",
      "  230  367    2  263  353    3    1   51   67    3   52    7  199   38\n",
      "    1 1068    4    1 3535 1068    4    1  896    1 2138   20  298    1\n",
      " 1068    4    1  896    6    1 2137  331  896    2   35   20    1  905\n",
      " 5277    9   32  263 3139    2    1 5278    4  292  106  436  645  436\n",
      "   20    1 3103    9  292  106   11   18  645  436  150   23  762  405\n",
      "  251 1313 3536 5279    2 1182   89    1  292  106    6   78 1161    4\n",
      "    1  585   10  767    6 2113    2 2670    1  330   11   18   68  235\n",
      "    1  645  436    9    1  159  106  235    1   44    2  117    2  136\n",
      " 1202   28   38    1  178   57 2671    1  330    6    1 1474    4 5280\n",
      " 5281 3492 2671    1  405  762    2 1298    6    1 1474    4 3461 5282\n",
      " 1065 3042  107  645  436   20 3530    2    1  330   10 3114  439    3\n",
      "  602 3537  360    4  206  107  581    3    1  360    4  206  150   23\n",
      "  388 1194  247    2 1450  206 3168   94  469    2  731  206 1424    3\n",
      "  595  560    2  383    6  251  142  330 1067 1065    2 2672    2  624\n",
      "    9  619  642  206   18  333    1  108 1718 5283  642 1494    6  532\n",
      "  247 5284  117   31  333    1  578 3534  251 2140  247 3538    2 1450\n",
      "  206   31  333    1  388 5285 3538 2140  115    2  106   10 1473 2645\n",
      "  235    1  360    4  206   64   14  143    1  469    2  731  560    3\n",
      "    1 1333   96   67    3  665    7   73  206 2092    1   74 1353  235\n",
      "  299    2  401  206    2 1716  791  134  415  299    2  401  299 2134\n",
      "  344    2 5286  102  401  626    2  136  206  147 2135    2 1043 2628\n",
      "   31 3154   60 1709  581    3    1  360    4 2673 2674  206   16 1183\n",
      "    3 5287    2  835    1  645  436 1183    3  304    9 1636    1  330\n",
      "    2 5288    1 2672    1  299 2675    2  136 2132   14  759   94  203\n",
      "  299    2 3523  383   14 3485  567   27    1  405    2  511  102    1\n",
      " 2135  626    2 2538  206    1 5289 5290    2 2673 2674  206  235 1043\n",
      "  626  128 1924    2 2667    1  567   27 1164 1905 1176 3112   19  405\n",
      "  251    2  762 1286    2  466    4    1 1298 1307  102    1 2311  150\n",
      "   23 5291    2 5292 1652 2673 2674 5293  770  128 2667    1  567   27\n",
      "  567 2675    4    1 1118    4  567 1299   64  128 1714    1  203   19\n",
      "   67    3  230  292  106    6  595  191  150   23 1065 1067  142 5294\n",
      "    2 2672  235    1  299    2  401  206   11   82    3 1331 1179  791\n",
      "  134  415   47    7    1  115  631    4  360    4  206   20  460    9\n",
      "  159  106    6    5  292  141  791  134  415   38   41  434  117  276\n",
      "  383  106 1461   74    4  156  115    2   38  113  434 1716   43 1179\n",
      "  134  415   64   14  123    1  206    3    1 2138    9  566  292  106\n",
      "  383 3497  263 1202   27    1  146  791  134  415   64   14 1072    2\n",
      " 1714    1 2138  150   23  195 2408 2625  394   46  413    6    1  245\n",
      "  191   40    1  292  106  791  134  415   10  788    7   14 1186   94\n",
      "    1  290    4  908  645  436 3532 5295   15  242 2671    1  330    2\n",
      "   38    1  178   57  100   15  908    9    1  747    3  114   47    7\n",
      "  602  367    2  353   10 1935]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[11], '\\n\\n', data[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default `padding='pre'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[915, 3695, 662, 1610, 448, 865, 26, 182, 1552, 448, 14, 153, 2, 29, 153, 7227, 1533, 865, 1470, 1227, 67, 8, 826, 53, 1278, 1475, 138, 138, 10, 47, 1408, 7228, 2, 7229, 1, 168, 7230, 7231, 7, 8, 18, 268, 305, 6, 26, 816, 85, 2, 67, 30, 268, 343, 94, 325, 2, 3, 1, 1265, 6, 1, 108, 252, 10, 47, 7232, 3, 54, 15, 10, 5, 7233, 2485, 3, 26, 7234, 2507, 1070, 138, 10, 24, 6, 173, 141, 5, 48, 687, 295, 2, 167, 7235, 2, 2322, 1912, 56, 24, 160, 15, 1464, 70, 35, 174, 184, 37, 31, 210, 4, 36, 96, 8, 407, 5, 923, 22, 350, 54, 7, 99, 10, 221, 3, 698, 9, 138, 8, 350, 105, 7, 138, 10, 5, 7236, 7237, 2, 7, 30, 7238, 7239, 6, 1, 7240, 2, 7, 30, 743, 3, 4161, 36, 2, 7, 30, 7241, 36, 194, 2, 939, 8, 350, 105, 7, 138, 10, 5, 3783, 2, 7, 30, 743, 3, 230, 6, 3993, 607, 2, 7, 30, 14, 230, 42, 32, 94, 3299, 28, 99, 248, 24, 160, 54, 99, 375, 7, 8, 33, 3925, 3, 21, 2418, 5, 943, 1166, 7242, 165, 46, 14, 83, 24, 268, 336, 865, 89, 34, 414, 27, 1, 7243, 1596, 1091, 7244, 129, 94, 1, 194, 333, 411, 587, 7, 35, 95, 7245, 3, 21, 181, 17, 37, 52, 11, 52, 17, 46, 22, 3917, 3, 1372, 8, 52, 24, 109, 3, 21, 265, 1, 60, 58, 1437, 39, 907, 94, 1044, 1281, 83, 66, 47, 76, 46, 109, 3, 4161, 85, 9, 1, 1107, 4, 42, 194, 275, 7246, 943, 2, 279, 7247] \n",
      "\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0  915 3695  662 1610  448  865   26\n",
      "  182 1552  448   14  153    2   29  153 7227 1533  865 1470 1227   67\n",
      "    8  826   53 1278 1475  138  138   10   47 1408 7228    2 7229    1\n",
      "  168 7230 7231    7    8   18  268  305    6   26  816   85    2   67\n",
      "   30  268  343   94  325    2    3    1 1265    6    1  108  252   10\n",
      "   47 7232    3   54   15   10    5 7233 2485    3   26 7234 2507 1070\n",
      "  138   10   24    6  173  141    5   48  687  295    2  167 7235    2\n",
      " 2322 1912   56   24  160   15 1464   70   35  174  184   37   31  210\n",
      "    4   36   96    8  407    5  923   22  350   54    7   99   10  221\n",
      "    3  698    9  138    8  350  105    7  138   10    5 7236 7237    2\n",
      "    7   30 7238 7239    6    1 7240    2    7   30  743    3 4161   36\n",
      "    2    7   30 7241   36  194    2  939    8  350  105    7  138   10\n",
      "    5 3783    2    7   30  743    3  230    6 3993  607    2    7   30\n",
      "   14  230   42   32   94 3299   28   99  248   24  160   54   99  375\n",
      "    7    8   33 3925    3   21 2418    5  943 1166 7242  165   46   14\n",
      "   83   24  268  336  865   89   34  414   27    1 7243 1596 1091 7244\n",
      "  129   94    1  194  333  411  587    7   35   95 7245    3   21  181\n",
      "   17   37   52   11   52   17   46   22 3917    3 1372    8   52   24\n",
      "  109    3   21  265    1   60   58 1437   39  907   94 1044 1281   83\n",
      "   66   47   76   46  109    3 4161   85    9    1 1107    4   42  194\n",
      "  275 7246  943    2  279 7247]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[33], '\\n\\n', data[33])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.2, random_state = 123, \n",
    "                                                     stratify = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert target into one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Embedding layer\n",
    "\n",
    "Compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open(os.path.join(PATH, 'glove.6b.50d.txt'), encoding=\"utf8\") as f :\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7314, 50)\n"
     ]
    }
   ],
   "source": [
    "embedding_Matrix = np.zeros((vocab_Size, 50))\n",
    "\n",
    "for word, i in word_Index.items():\n",
    "    \n",
    "    embedding_Vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_Vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_Matrix[i] = embedding_Vector\n",
    "\n",
    "print (embedding_Matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embedding layer\n",
    "\n",
    "The embedding layer can be seeded with the GloVe word embedding weights. \n",
    "\n",
    "- We chose the 50-dimensional version, therefore the Embedding layer must be defined with output_dim set to 50. \n",
    "- We do not want to update the learned word weights in this model, therefore we will set the trainable attribute for the model to be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input( shape = (MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedded_sequences = Embedding(vocab_Size, 50, weights = [embedding_Matrix], \n",
    "                               trainable = False)(sequence_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build 1D convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(64, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(64, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(4)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "preds = Dense(len(TEXT_DATA_DIR_LIST), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 50)          365700    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 64)           16064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 245, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 57, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 14, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                57408     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 480,455\n",
      "Trainable params: 114,755\n",
      "Non-trainable params: 365,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "28/28 [==============================] - 1s 48ms/step - loss: 1.1858 - acc: 0.3571\n",
      "Epoch 2/25\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1.3307 - acc: 0.3571\n",
      "Epoch 3/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 1.0880 - acc: 0.3571\n",
      "Epoch 4/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.9098 - acc: 0.4643\n",
      "Epoch 5/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.8711 - acc: 0.7143\n",
      "Epoch 6/25\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8617 - acc: 0.5714\n",
      "Epoch 7/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.7790 - acc: 0.7500\n",
      "Epoch 8/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.6797 - acc: 1.0000\n",
      "Epoch 9/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.6178 - acc: 1.0000\n",
      "Epoch 10/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.5623 - acc: 1.0000\n",
      "Epoch 11/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.4943 - acc: 1.0000\n",
      "Epoch 12/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.4206 - acc: 1.0000\n",
      "Epoch 13/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.3566 - acc: 1.0000\n",
      "Epoch 14/25\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3060 - acc: 1.0000\n",
      "Epoch 15/25\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.2526 - acc: 1.0000\n",
      "Epoch 16/25\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.2032 - acc: 1.0000\n",
      "Epoch 17/25\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1695 - acc: 1.0000\n",
      "Epoch 18/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.1365 - acc: 1.0000\n",
      "Epoch 19/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.1049 - acc: 1.0000\n",
      "Epoch 20/25\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0831 - acc: 1.0000\n",
      "Epoch 21/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0673 - acc: 1.0000\n",
      "Epoch 22/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0505 - acc: 1.0000\n",
      "Epoch 23/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0381 - acc: 1.0000\n",
      "Epoch 24/25\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0307 - acc: 1.0000\n",
      "Epoch 25/25\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0243 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2c04b6d8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "8/8 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5816278457641602, 0.625]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6966944e-02 6.7747629e-01 2.7555683e-01]\n",
      " [9.0951747e-01 7.1693525e-02 1.8788939e-02]\n",
      " [5.4703359e-02 5.6427693e-01 3.8101968e-01]\n",
      " [2.6960969e-02 5.5746186e-01 4.1557717e-01]\n",
      " [9.9486887e-01 4.9004219e-03 2.3066011e-04]\n",
      " [6.0076278e-02 7.0722181e-01 2.3270196e-01]\n",
      " [5.7064582e-02 4.9829453e-01 4.4464085e-01]\n",
      " [1.3497594e-01 5.8634394e-01 2.7868021e-01]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred =[]\n",
    "\n",
    "for i in Y_pred:\n",
    "    y_pred.append(np.argmax(i))\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 2, 0, 1, 1, 2]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 3, 0],\n",
       "       [0, 3, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 50)          365700    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 1000, 256)         314368    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                82176     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 762,439\n",
      "Trainable params: 396,739\n",
      "Non-trainable params: 365,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(256, return_sequences = True)(embedded_sequences)\n",
    "lstm = LSTM(64)(lstm)\n",
    "preds = Dense(3, activation='softmax')(lstm)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "28/28 [==============================] - 11s 403ms/step - loss: 1.0953 - acc: 0.3214\n",
      "Epoch 2/25\n",
      "28/28 [==============================] - 7s 263ms/step - loss: 1.0509 - acc: 0.5000\n",
      "Epoch 3/25\n",
      "28/28 [==============================] - 8s 278ms/step - loss: 0.9809 - acc: 0.5000\n",
      "Epoch 4/25\n",
      "28/28 [==============================] - 6s 226ms/step - loss: 0.9130 - acc: 0.7143\n",
      "Epoch 5/25\n",
      "28/28 [==============================] - 6s 225ms/step - loss: 0.8371 - acc: 0.8214\n",
      "Epoch 6/25\n",
      "28/28 [==============================] - 7s 268ms/step - loss: 0.7292 - acc: 0.8214\n",
      "Epoch 7/25\n",
      "28/28 [==============================] - 7s 249ms/step - loss: 0.6482 - acc: 0.7143\n",
      "Epoch 8/25\n",
      "28/28 [==============================] - 6s 230ms/step - loss: 0.6766 - acc: 0.5714\n",
      "Epoch 9/25\n",
      "28/28 [==============================] - 6s 220ms/step - loss: 0.4971 - acc: 0.8929\n",
      "Epoch 10/25\n",
      "28/28 [==============================] - 6s 231ms/step - loss: 0.4509 - acc: 0.8571\n",
      "Epoch 11/25\n",
      "28/28 [==============================] - 6s 213ms/step - loss: 0.4251 - acc: 0.7500\n",
      "Epoch 12/25\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 0.3843 - acc: 0.8929\n",
      "Epoch 13/25\n",
      "28/28 [==============================] - 6s 211ms/step - loss: 0.3399 - acc: 1.0000\n",
      "Epoch 14/25\n",
      "28/28 [==============================] - 6s 212ms/step - loss: 0.3204 - acc: 0.8929\n",
      "Epoch 15/25\n",
      "28/28 [==============================] - 6s 209ms/step - loss: 0.2942 - acc: 0.9643\n",
      "Epoch 16/25\n",
      "28/28 [==============================] - 6s 211ms/step - loss: 0.2511 - acc: 1.0000\n",
      "Epoch 17/25\n",
      "28/28 [==============================] - 6s 219ms/step - loss: 0.2345 - acc: 0.9286\n",
      "Epoch 18/25\n",
      "28/28 [==============================] - 6s 214ms/step - loss: 0.1730 - acc: 1.0000\n",
      "Epoch 19/25\n",
      "28/28 [==============================] - 6s 211ms/step - loss: 0.1784 - acc: 0.9643\n",
      "Epoch 20/25\n",
      "28/28 [==============================] - 6s 208ms/step - loss: 0.1421 - acc: 0.9643\n",
      "Epoch 21/25\n",
      "28/28 [==============================] - 6s 211ms/step - loss: 0.1943 - acc: 0.9286\n",
      "Epoch 22/25\n",
      "28/28 [==============================] - 6s 210ms/step - loss: 0.1049 - acc: 0.9643\n",
      "Epoch 23/25\n",
      "28/28 [==============================] - 6s 214ms/step - loss: 0.0713 - acc: 1.0000\n",
      "Epoch 24/25\n",
      "28/28 [==============================] - 6s 204ms/step - loss: 0.1721 - acc: 0.9643\n",
      "Epoch 25/25\n",
      "28/28 [==============================] - 5s 194ms/step - loss: 0.1619 - acc: 0.9286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a37110710>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "8/8 [==============================] - 1s 185ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.226965308189392, 0.625]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00377436 0.9438871  0.05233848]\n",
      " [0.99316347 0.0048024  0.00203409]\n",
      " [0.00510955 0.08603683 0.9088536 ]\n",
      " [0.00329399 0.97817296 0.01853301]\n",
      " [0.02031444 0.96698475 0.0127009 ]\n",
      " [0.00348596 0.9583928  0.03812126]\n",
      " [0.01057569 0.6052689  0.3841554 ]\n",
      " [0.0060836  0.70055145 0.293365  ]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred =[]\n",
    "\n",
    "for i in Y_pred:\n",
    "    y_pred.append(np.argmax(i))\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 2, 0, 1, 1, 2]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0],\n",
       "       [0, 3, 0],\n",
       "       [0, 2, 1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
